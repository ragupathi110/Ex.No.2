
# Ex.No: 2 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta 

# DATE : 12:04:2025

# REGISTER NUMBER : 212222060185

## Aim:

To assess the effectiveness, usability, and response quality of various prompting tools across popular Al platforms-ChatGPT, Claude, Bard, Cohere Command, and Meta-in a specific use case (e.g.. summarizing text or answering technical questions).

## Procedure:

1. Define the Use Case:
   
•	Select a specific task for comparison across platforms, such as:

•	Summarizing a lengthy technical document

•	Answering complex technical questions

•	Generating creative content based on specific prompts

2. Design Standardized Prompts:

Create a set of standardized prompts tailored to the selected use case. For example:

•	Summarization Prompt: "Summarize the following document about machine learning techniques.

•	Technical Q&A Prompt: "Explain the concept of backpropagation in neural networks."

•	Creative Writing Prompt: "Generate a short story about an Al robot in a futuristic city."

3. Input Prompts into Each Platform:

•	Input each standardized prompt into ChatGPT, Claude, Bard, Cohere Command, and Meta Al platforms.

•	Record the responses generated by each platform.

4. Evaluate Response Quality:

Assess the responses from each platform using criteria such as:

•	Accuracy: Correctness of information and adherence to prompt requirements.

•	Clarity: Ease of understanding and logical structure.

•	Depth: Completeness of the response, especially for technical explanations.

•	Creativity: (if applicable) Originality and engagement in responses for creative prompts.

•	Rate each response on a scale from 1 to 5 based on these criteria.


5. Measure User Experience:

Assess user experience aspects like:

•	Response Speed: Time taken to generate a response.

•	Interface Usability: Ease of use and intuitiveness of the platform's interface.

•	Prompt Customization: Ability to modify prompts easily or clarify questions.

•	Interaction Features: (e.g., follow-up questions, adaptive responses, or customization options).

6. Compare Consistency Across Multiple Queries:

•	Test each platform's performance consistency by running multiple prompts within the same use case. Record any fluctuations in response quality, accuracy, or usability.

## Outcomes:

## Prompt: "Explain convolutional neural networks (CNNs) and their applications in image processing."


![image](https://github.com/user-attachments/assets/58a5e861-ccfc-433e-8905-3873fd9061c8)

![image](https://github.com/user-attachments/assets/d53b2cf7-5c93-4e61-b5af-8a76fa9c294c)

![image](https://github.com/user-attachments/assets/08efc977-8fde-48d3-8add-a4a176abc2cf)

![image](https://github.com/user-attachments/assets/c077f837-cdcf-4e21-8c90-a6bd77e19706)


## CONCLUSION:

The comparison reveals strength and weakness of each Al platform in the chosen use case.while some platforms xccel in accuracy and depth for technical prompts(like ChatGPT ,gemini, copilot), other may outperform in creativity for storytelling (e.g.meta).

Factors like interface usability and response customization options also influence the user experience with tool like gemini, meta offering flexiable prompt modification which can be beneficial for refining response.

This evaluation provides a comprehensive understanding of each platform's capabilities and ideal use case, guiding users in selecting the best tool based on specific needs, whether technical creative, and inactive.



